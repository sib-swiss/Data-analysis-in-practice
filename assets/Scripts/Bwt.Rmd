---
title: "Linear regression (Birth weight data)"
author: "Rachel Marcone"
date: "04/04/2023"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## File import

```{r}
library(MASS)

data(birthwt)
summary(birthwt)

help(birthwt)

colnames(birthwt)
colnames(birthwt) <- c("birthwt.below.2500", "mother.age","mother.weight", "race",
                       "smoking.status", "nb.previous.prem.labor",  "hypertension", 
                       "uterine.irrit","nb.physician.visits", "birthwt.grams")

str(birthwt)
summary(birthwt)
birthwt$race <- as.factor(birthwt$race)
birthwt$smoking.status <- as.factor(birthwt$smoking.status)
birthwt$nb.physician.visits <- as.factor(birthwt$nb.physician.visits)
birthwt$uterine.irrit  <- as.factor(birthwt$uterine.irrit )

birthwt$hypertension  <- as.factor(birthwt$hypertension )

birthwt$nb.previous.prem.labor  <- as.factor(birthwt$nb.previous.prem.labor )

str(birthwt)
summary(birthwt)
```



## Model to predict birth weight using mother's age

```{r}
plot(birthwt$mother.age, birthwt$birthwt.grams, ylab = "Birth Weight (in grams)", xlab="Mother's Age")

lm1 <- lm(birthwt.grams ~ mother.age, data = birthwt)
lm1
summary(lm1)

abline(lm1, col="red", lwd=2)
```



## Regression assumptions

Linear regression makes several assumptions about the data, such as :

 * Linearity of the data. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.
 * Normality of residuals. The residual errors are assumed to be normally distributed: Ei ~ N(0,V)
 * Homogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity): V(Ei) = V 
 * Independence of residuals error terms: Ei are independent from Xi and mutually independent

You should check whether or not these assumptions hold true. Potential problems include:

 * Non-linearity of the outcome - predictor relationships
 * Heteroscedasticity: Non-constant variance of error terms (residuals).
 * Presence of influential values in the data that can be:
    + Outliers: extreme values in the outcome (y) variable
    + High-leverage points: extreme values in the predictors (x) variable

All these assumptions and potential problems can be checked by producing some diagnostic plots visualizing the residual errors.
Regression diagnostics plots can be created using the R base function plot().
The diagnostic plots show residuals in four different ways:

 * Residuals vs Fitted. Used to check the linear relationship assumptions. 
   A horizontal line, without distinct patterns is an indication for a linear relationship.

```{r}
plot(lm1, 1)
```

Ideally, the red line should be approximately horizontal at zero.
The presence of a pattern may indicate a problem with some aspect of the linear model.
In our example, our oldest mother and her heaviest child are greatly skew this analysis 

```{r}
birthwt.noout <- birthwt[birthwt$mother.age <= 40,]
lm2 <-lm(birthwt.grams ~ mother.age, data=birthwt.noout)
lm2
summary(lm2)

plot(lm2, 1)
```

 * Normality of residuals. 

```{r}
plot(lm2, 2)
```

In our example, all the points fall approximately along this reference line, so we can assume normality.

```{r}
shapiro.test(lm2$residuals)
```

p-value > 0.05: the distribution is not significantly different from normal distribution

 * Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). 
Horizontal line with equally spread points is a good indication of homoscedasticity. 

```{r}
plot(lm2, 3)
```

In our example, the residuals are spread equally along the ranges of predictors.
We see a horizontal line with equally spread points
A standardized residual (or studentized residual) is a residual divided by its estimated standard error. 

 * Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis.

Cook's distance measures how much the predicted values from the regression would change 
if each observation were removed from the data used to fit the model. 
A rule of thumb is that an observation has high influence if Cook's distance exceeds 4/n 
where n is the number of observations.
By default, the top 3 most extreme values are labelled on the Cook's distance plot. 

```{r}
plot(lm2, 4)
plot(lm2, 4, id.n = 8)
abline(a = 4/dim(birthwt.noout)[1], b = 0)
```

Observations whose standardized residuals are greater than 3 in absolute value are possible outliers (James et al. 2014).
The Residuals vs Leverage plot can help us to find influential observations if any. 
On this plot, outlying values are generally located at the upper right corner or at the lower right corner. 
Those spots are the places where data points can be influential against a regression line.

```{r}
plot(lm2, 5)
```

When data points have high Cook's distance scores and are to the upper or lower right of the leverage plot, 
they have leverage meaning they are influential to the regression results. The regression results will 
be altered if we exclude those cases. 
In our example, the data don't present any influential points. 
Cook's distance lines (a red dashed line) are not shown on the Residuals vs Leverage plot because 
all points are well inside of the Cook's distance lines.

The diagnostic for potential problems in the linear model is essentially performed by visualizing the residuals. 
Having patterns in residuals is not a stop signal. 
Our current regression model might not be the best way to understand your data.
Potential problems might be:

 * A non-linear relationships between the outcome and the predictor variables.
   When facing to this problem, one solution is to include a quadratic term, such as polynomial terms or log           transformation.
 * Existence of important variables that you left out from your model. 
   Other variables you didn't include (e.g. smoking status) may play an important role in your model and data.
 * Presence of outliers. If you believe that an outlier has occurred due to an error in data collection and entry, 
   then one solution is to simply remove the concerned observation.



## Model to predict birth weight using all other variables in the dataset

```{r}
lm3 <- lm(birthwt.grams ~ . - birthwt.below.2500, data = birthwt)
lm3
summary(lm3)

plot(lm3, 1)
plot(lm3, 2)
plot(lm3, 3)
plot(lm3, 4)
plot(lm3, 5)
```



## Model to predict birth weight using several variables in the dataset

```{r}
lm4 <- lm(birthwt.grams ~ mother.weight+smoking.status+hypertension+uterine.irrit, data = birthwt)
lm4
summary(lm4)

plot(lm4, 1)
plot(lm4, 2)
plot(lm4, 3)
plot(lm4, 4)
plot(lm4, 5)
```



## Model selection

```{r}
library(leaps) 

best_subset <- regsubsets(birthwt.grams ~ . - birthwt.below.2500, data = birthwt, nvmax = 8)
results <- summary(best_subset)
```

Adjusted R-squared

```{r}
plot(results$adjr2, xlab = "Number of Variables", ylab = "Adjusted R-squared", type = "l")
```

Residual sum of squares for each model

```{r}
plot(results$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
```

R-squared

```{r}
plot(results$rsq, xlab = "Number of Variables", ylab = "R-squared", type = "l")

which.max(results$adjr2)
```



## Model selection using the validation set approach

```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), size = nrow(birthwt), rep = TRUE)
test <- (!train)

best_subset_train <- regsubsets(birthwt.grams ~ . - birthwt.below.2500, data = birthwt[train ,], nvmax = 8)
test_mat <- model.matrix(birthwt.grams ~ . - birthwt.below.2500, data = birthwt[test,])

val_errors  <- rep(NA , 8)
for(i in 1:8){
  coefi = coef(best_subset_train, id = i)
  pred = test_mat[,names(coefi)]%*%coefi
  val_errors[i] = mean((birthwt$birthwt.grams[test] - pred)^2)
}
which.min(val_errors)
```



## Estimating the parameters of the linear regression model using the best model and the full dataset

```{r}
lm5 <- lm(birthwt.grams ~ mother.weight+race+smoking.status+hypertension+uterine.irrit, data = birthwt)
lm5
summary(lm5)

plot(lm5, 1)
plot(lm5, 2)
plot(lm5, 3)
plot(lm5, 4)
plot(lm5, 5)
```